<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Guide &middot; MALT-2
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-08">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          MALT-2
        </a>
      </h1>
      <p class="lead">Data-parallel machine learning for Torch/MiLDE with CUDA/MPI with support for infiniBand and NVLINK.</p>
    </div>

    <nav class="sidebar-nav">
     <a class="sidebar-nav-item" href="/">Home</a> 

      

      
      
        
          
        
      
        
      
        
          
            <a class="sidebar-nav-item active" href="/guide/">Guide</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/lua-apps/">Applications</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/qa/">FAQs</a>
          
        
      

      <!--<a class="sidebar-nav-item" href="http://github.com/malt2/malt2.github.io/archive/v2.0.1.zip">Examples</a> -->
      <a class="sidebar-nav-item" href="https://github.com/malt2/malt2">github link</a>
      <span class="sidebar-nav-item">Currently v2.0.1</span>
    </nav>

    <p>&copy; 2017. NEC Labs.</p>
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Guide</h1>
  <p class="message">
This page describes how to quickly get started with MALT-2. MALT-2
 parallelizes Torch over multiple CPUs and GPUs.
</p>

<h2>Building MALT with Torch</h2>

<h3>Requirements</h3>

<ul>
<li><a href="http://torch.ch">Torch</a></li>
<li><a href="https://www.open-mpi.org/">MPI (OpenMPI or MPICH)</a></li>
<li><a href="http://www.boost.org/">Boost (1.54 or higher)</a> </li>
</ul>

<h3>Setup</h3>

<h3>Install Torch, MPI, Boost and CUDA (if using GPU).</h3>

<ul>
<li>Checkout the latest version of MALT-2 from <a href="https://github.com/malt-2">github</a></li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>git clone https://github.com/malt2/malt2.git --recursive</code></pre></figure>

<h3>Setup the environment variables</h3>

<h3>Source your torch/cuda/MKL environment:</h3>

<p>on some machines, you might need things something like (MKL is optional):</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="nb">source</span> <span class="o">[</span>torch-dir<span class="o">]</span>/install/bin/torch-activate
<span class="nb">source</span> /opt/intel/mkl/bin/intel64/mklvars.sh intel64</code></pre></figure>

<p>If using modules, you can try:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>module install icc cuda80 luajit</code></pre></figure>

<h3>To build everything including dstorm, orm and torch, just type from the top-level directory:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>make</code></pre></figure>

<p>This command builds the distributed shared memory component (dstorm), the shared memory transport hook (orm)
and the luarocks for torch hooks and distributed optimization.</p>

<h3>Component-wise build</h3>

<p>To build componenet-wise (not required if using make above):</p>

<h4>Build the dstorm directory, run:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="nb">cd</span> dstorm
./mkit.sh GPU <span class="nb">test</span></code></pre></figure>

<p>You should get a <code>SUCCESS</code> as the output. Check the log files to ensure the build is successful.</p>

<p>The general format is:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>./mkit.sh &lt;type&gt; </code></pre></figure>

<p>where TYPE is: 
          or MPI (liborm  + mpi)
          or GPU (liborm + mpi + gpu)
A side effect is to create ../dstorm-env.{mk|cmake} environment files, so lua capabilities
can match the libdstorm compile options.</p>

<h4>Build the orm</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="nb">cd</span> orm
./mkorm.sh GPU</code></pre></figure>

<h4>Building Torch packages. With Torch environment setup, install the malt-2 and dstoptim (distributed optimization packages)</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="nb">cd</span> dstorm/src/torch
rm -rf build <span class="o">&amp;&amp;</span> <span class="nv">VERBOSE</span><span class="o">=</span><span class="m">7</span> luarocks make malt-2-scm-1.rockspec &gt;<span class="p">&amp;</span> mk.log <span class="o">&amp;&amp;</span> <span class="nb">echo</span> YAY <span class="c1">#build and install the malt-2 package</span>
<span class="nb">cd</span> dstoptim
rm -rf build <span class="o">&amp;&amp;</span> <span class="nv">VERBOSE</span><span class="o">=</span><span class="m">7</span> luarocks make dstoptim-scm-1.rockspec &gt;<span class="p">&amp;</span>mk.log <span class="o">&amp;&amp;</span> <span class="nb">echo</span> YAY <span class="c1"># build the dstoptim package</span></code></pre></figure>

<h3>Test</h3>

<ul>
<li>A very basic test is to run th and then try, by hand,</li>
</ul>

<figure class="highlight"><pre><code class="language-lua" data-lang="lua"><span></span><span class="nb">require</span> <span class="s2">&quot;malt2&quot;</span></code></pre></figure>

<h3>Run a quick test.</h3>

<ul>
<li>With MPI, then you&#39;ll need to run via mpirun, perhaps something like:</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>mpirun -np <span class="m">3</span> <span class="sb">`</span>which th<span class="sb">`</span> <span class="sb">`</span><span class="nb">pwd</span> -P<span class="sb">`</span>/test.lua mpi <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee test-mpi.log</code></pre></figure>

<ul>
<li>if GPU,</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>mpirun -np <span class="m">3</span> <span class="sb">`</span>which th<span class="sb">`</span> <span class="sb">`</span><span class="nb">pwd</span> -P<span class="sb">`</span>/test.lua gpu <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee test-GPU-gpu.log</code></pre></figure>

<ul>
<li>NEW: a <code>WITH_GPU</code> compile can also run with MPI transport</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>mpirun -np <span class="m">3</span> <span class="sb">`</span>which th<span class="sb">`</span> <span class="sb">`</span><span class="nb">pwd</span> -P<span class="sb">`</span>/test.lua mpi <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee test-GPU-mpi.log</code></pre></figure>

<p>default transport is set to the &quot;highest&quot; built into libdstorm2: GPU &gt; MPI  &gt; SHM</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>mpirun -np <span class="m">3</span> <span class="sb">`</span>which th<span class="sb">`</span> <span class="sb">`</span><span class="nb">pwd</span> -P<span class="sb">`</span>/test.lua <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee test-best.log</code></pre></figure>

<h3>Running over multiple GPUs.</h3>

<ul>
<li><p>MPI only sees the hostname. By default, on every host, MPI jobs enumerate the
GPUs and start running the processes. The only way to change this and run on
other GPUs in a round-robin fashion is to change this enumeration for every
rank using <code>CUDA_VISIBLE_DEVICES</code>. An example script is in <code>redirect.sh</code> file
in the top-level directory.</p></li>
<li><p>To run:</p></li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>mpirun -np <span class="m">2</span> ./redirect.sh <span class="sb">`</span>which th<span class="sb">`</span> <span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>/test.lua</code></pre></figure>

<p>This script assigns available GPUs in a round-robin fashion. Since MPI requires
visibility of all other GPUs to correctly access shared memory, this script only
changes the enumeration order and does not restrict visibility.</p>

<h3>Running applications.</h3>

<ul>
<li>Check out <a href="../lua-apps">here</a> to see how to run Torch applications. </li>
</ul>

</div>

    </div>

  </body>
</html>
